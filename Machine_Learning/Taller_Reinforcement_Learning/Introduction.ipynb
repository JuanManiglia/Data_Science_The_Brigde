{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the scenario of teaching a dog new tricks. The dog doesn't understand our language, so we can't tell him what to do. Instead, we follow a different strategy. We emulate a situation (or a cue), and the dog tries to respond in many different ways. If the dog's response is the desired one, we reward them with snacks. Now guess what, the next time the dog is exposed to the same situation, the dog executes a similar action with even more enthusiasm in expectation of more food. That's like learning \"what to do\" from positive experiences. Similarly, dogs will tend to learn what not to do when face with negative experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly how Reinforcement Learning works in a broader sense:\n",
    "\n",
    "* Your dog is an \"agent\" that is exposed to the environment. The environment could in your house, with you.\n",
    "* The situations they encounter are analogous to a state. An example of a state could be your dog standing and you use a specific word in a certain tone in your living room\n",
    "* Our agents react by performing an action to transition from one \"state\" to another \"state,\" your dog goes from standing to sitting, for example.\n",
    "* After the transition, they may receive a reward or penalty in return. You give them a treat! Or a \"No\" as a penalty.\n",
    "* The policy is the strategy of choosing an action given a state in expectation of better outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning lies between the spectrum of Supervised Learning and Unsupervised Learning, and there's a few important things to note:\n",
    "\n",
    "Being greedy doesn't always work\n",
    "* There are things that are easy to do for instant gratification, and there's things that provide long term rewards The goal is to not be greedy by looking for the quick immediate rewards, but instead to optimize for maximum rewards over the whole training.\n",
    "* Sequence matters in Reinforcement Learning\n",
    "The reward agent does not just depend on the current state, but the entire history of states. Unlike supervised and unsupervised learning, time is important here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Reinforcement-Learning-Animation.gif\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a way, Reinforcement Learning is the science of making optimal decisions using experiences. Breaking it down, the process of Reinforcement Learning involves these simple steps:\n",
    "\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act using some strategy\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. Learning from the experiences and refining our strategy\n",
    "6. Iterate until an optimal strategy is found\n",
    "\n",
    "Let's now understand Reinforcement Learning by actually developing an agent to learn to play a game automatically on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\n",
    "\n",
    "The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you’ll need to have Python 3.5+ installed. Simply install gym using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a bare minimum example of getting something running. This will run an instance of the CartPole-v0 environment for 1000 timesteps, rendering the environment at each step. You should see a window pop up rendering the classic cart-pole problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for i in range(300):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for i in range(600):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we’ll end the simulation before the cart-pole is allowed to go off-screen. More on that later. For now, please ignore the warning about calling step() even though this environment has already returned done = True.\n",
    "\n",
    "If you’d like to see some other environments in action, try replacing CartPole-v0 above with something like MountainCar-v0, MsPacman-v0 (requires the Atari dependency), or Hopper-v1 (requires the MuJoCo dependencies). Environments all descend from the Env base class.\n",
    "\n",
    "Note that if you’re missing any dependencies, you should get a helpful error message telling you what you’re missing. (Let us know if a dependency gives you trouble without a clear instruction to fix it.) Installing a missing dependency is generally pretty simple. You’ll also need a MuJoCo license for Hopper-v1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ever want to do better than take random actions at each step, it’d probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment’s step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "* observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "* reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "* done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "* info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an action, and the environment returns an observation and a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process gets started by calling reset(), which returns an initial observation. So a more proper way of writing the previous code would be to respect the done flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04814275 -0.0376203  -0.01749686  0.01238456]\n",
      "[-0.04889516 -0.23248701 -0.01724917  0.29949605]\n",
      "[-0.0535449  -0.03712349 -0.01125925  0.00142338]\n",
      "[-0.05428737  0.15815811 -0.01123078 -0.2947906 ]\n",
      "[-0.0511242   0.35343835 -0.0171266  -0.5909943 ]\n",
      "[-0.04405544  0.5487959  -0.02894648 -0.8890225 ]\n",
      "[-0.03307952  0.3540784  -0.04672693 -0.6055778 ]\n",
      "[-0.02599795  0.54982156 -0.05883849 -0.91260475]\n",
      "[-0.01500152  0.35554287 -0.07709058 -0.63897955]\n",
      "[-0.00789066  0.5516503  -0.08987018 -0.95490915]\n",
      "[ 0.00314234  0.74785864 -0.10896836 -1.2744204 ]\n",
      "[ 0.01809951  0.5542823  -0.13445677 -1.0177513 ]\n",
      "[ 0.02918516  0.7509154  -0.1548118  -1.3494501 ]\n",
      "[ 0.04420347  0.94760615 -0.1818008  -1.6862909 ]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.00499526  0.00483182  0.021218   -0.04613058]\n",
      "[ 0.00509189 -0.19058785  0.02029539  0.25317058]\n",
      "[ 0.00128014 -0.38599363  0.0253588   0.5521853 ]\n",
      "[-0.00643973 -0.19123682  0.0364025   0.26759872]\n",
      "[-0.01026447 -0.38685888  0.04175448  0.5715374 ]\n",
      "[-0.01800165 -0.19234654  0.05318523  0.29229543]\n",
      "[-0.02184858 -0.3881849   0.05903114  0.6012667 ]\n",
      "[-0.02961228 -0.19393623  0.07105647  0.32774627]\n",
      "[-0.033491   -0.38999406  0.07761139  0.6419639 ]\n",
      "[-0.04129088 -0.19603485  0.09045067  0.37469575]\n",
      "[-0.04521158 -0.00230631  0.09794459  0.11184711]\n",
      "[-0.04525771  0.19128568  0.10018153 -0.14839943]\n",
      "[-0.04143199 -0.00511753  0.09721354  0.17413238]\n",
      "[-0.04153434  0.18848842  0.10069619 -0.0863696 ]\n",
      "[-0.03776458 -0.00792196  0.0989688   0.23630825]\n",
      "[-0.03792302 -0.20430835  0.10369496  0.5584957 ]\n",
      "[-0.04200918 -0.01078311  0.11486488  0.30019882]\n",
      "[-0.04222484 -0.20733894  0.12086885  0.62678576]\n",
      "[-0.04637162 -0.01409295  0.13340457  0.37448058]\n",
      "[-0.04665348  0.17890677  0.14089417  0.12665796]\n",
      "[-0.04307535  0.37175864  0.14342734 -0.11846738]\n",
      "[-0.03564017  0.17490408  0.14105798  0.21580571]\n",
      "[-0.03214209  0.36775723  0.1453741  -0.0292672 ]\n",
      "[-0.02478695  0.17088209  0.14478876  0.30552012]\n",
      "[-0.0213693   0.36367592  0.15089916  0.06177466]\n",
      "[-0.01409579  0.16674875  0.15213466  0.3980049 ]\n",
      "[-0.01076081  0.359422    0.16009475  0.15688835]\n",
      "[-0.00357237  0.5519329   0.16323252 -0.08131819]\n",
      "[ 0.00746629  0.74438435  0.16160616 -0.31838053]\n",
      "[0.02235397 0.547374   0.15523854 0.02059013]\n",
      "[0.03330145 0.3504059  0.15565035 0.3579464 ]\n",
      "[0.04030957 0.54301214 0.16280928 0.11810631]\n",
      "[0.05116981 0.34597734 0.1651714  0.45740485]\n",
      "[0.05808936 0.5384262  0.1743195  0.22100024]\n",
      "[ 0.06885789  0.73068357  0.1787395  -0.01202453]\n",
      "[ 0.08347156  0.922852    0.17849901 -0.24341477]\n",
      "[ 0.1019286   1.1150349   0.17363071 -0.4749085 ]\n",
      "[ 0.1242293   1.3073347   0.16413255 -0.7082295 ]\n",
      "[ 0.15037599  1.4998486   0.14996795 -0.94508094]\n",
      "[ 0.18037295  1.3030596   0.13106634 -0.6092851 ]\n",
      "[ 0.20643415  1.1063726   0.11888064 -0.27836174]\n",
      "[0.2285616  0.909773   0.11331341 0.04932483]\n",
      "[ 0.24675706  1.1031032   0.1142999  -0.20556775]\n",
      "[0.26881912 0.906548   0.11018854 0.12087222]\n",
      "[ 0.28695008  1.0999329   0.11260599 -0.13511601]\n",
      "[0.30894876 0.9033932  0.10990367 0.19086263]\n",
      "[0.32701662 0.7068846  0.11372092 0.5160938 ]\n",
      "[0.3411543  0.5103604  0.12404279 0.8423379 ]\n",
      "[0.3513615  0.31378356 0.14088956 1.1713128 ]\n",
      "[0.35763717 0.50682104 0.1643158  0.9259103 ]\n",
      "[0.3677736  0.69938844 0.18283401 0.6890416 ]\n",
      "[0.38176137 0.8915657  0.19661485 0.45903468]\n",
      "[0.3995927  0.6942871  0.20579554 0.80668426]\n",
      "Episode finished after 53 timesteps\n",
      "[-0.04517885 -0.04812911  0.02136358  0.01742405]\n",
      "[-0.04614143  0.14668006  0.02171206 -0.2684426 ]\n",
      "[-0.04320784 -0.04874492  0.01634321  0.03100862]\n",
      "[-0.04418273  0.14613889  0.01696339 -0.25647336]\n",
      "[-0.04125996  0.3410146   0.01183392 -0.54375786]\n",
      "[-0.03443966  0.14572836  0.00095876 -0.24736995]\n",
      "[-0.03152509  0.3408366  -0.00398864 -0.5397503 ]\n",
      "[-0.02470836  0.14577095 -0.01478365 -0.24832681]\n",
      "[-0.02179294  0.34110087 -0.01975018 -0.54563594]\n",
      "[-0.01497093  0.14626193 -0.0306629  -0.25924066]\n",
      "[-0.01204569  0.3418079  -0.03584771 -0.5614352 ]\n",
      "[-0.00520953  0.53741413 -0.04707642 -0.865193  ]\n",
      "[ 0.00553875  0.34296346 -0.06438027 -0.5876755 ]\n",
      "[ 0.01239802  0.5389251  -0.07613379 -0.8999238 ]\n",
      "[ 0.02317652  0.73499167 -0.09413226 -1.2155334 ]\n",
      "[ 0.03787636  0.54120153 -0.11844293 -0.9537691 ]\n",
      "[ 0.04870038  0.73770064 -0.13751832 -1.2811936 ]\n",
      "[ 0.0634544   0.5445722  -0.16314219 -1.0345389 ]\n",
      "[ 0.07434584  0.7414429  -0.18383296 -1.3736751 ]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.01208031 -0.02344391 -0.02365617 -0.02566799]\n",
      "[-0.01254919 -0.21821876 -0.02416953  0.25945827]\n",
      "[-0.01691357 -0.02276026 -0.01898036 -0.04074899]\n",
      "[-0.01736877  0.17262866 -0.01979534 -0.33935946]\n",
      "[-0.0139162   0.36802658 -0.02658253 -0.6382184 ]\n",
      "[-0.00655567  0.1732852  -0.0393469  -0.35402384]\n",
      "[-0.00308996 -0.02125582 -0.04642738 -0.07400318]\n",
      "[-0.00351508 -0.21568249 -0.04790744  0.20367783]\n",
      "[-0.00782873 -0.41008776 -0.04383389  0.48087174]\n",
      "[-0.01603048 -0.21437535 -0.03421645  0.17470208]\n",
      "[-0.02031799 -0.01878083 -0.03072241 -0.12857564]\n",
      "[-0.02069361 -0.21344951 -0.03329392  0.15425862]\n",
      "[-0.0249626  -0.40807933 -0.03020875  0.43625495]\n",
      "[-0.03312419 -0.6027609  -0.02148365  0.7192641 ]\n",
      "[-0.0451794  -0.7975791  -0.00709837  1.0051082 ]\n",
      "[-0.06113099 -0.99260557  0.0130038   1.2955536 ]\n",
      "[-0.08098309 -1.1878903   0.03891487  1.592279  ]\n",
      "[-0.1047409  -0.99325114  0.07076045  1.3119794 ]\n",
      "[-0.12460592 -0.79909295  0.09700003  1.0422571 ]\n",
      "[-0.14058779 -0.6053837   0.11784518  0.7815324 ]\n",
      "[-0.15269546 -0.8019113   0.13347583  1.1088456 ]\n",
      "[-0.16873369 -0.60877156  0.15565273  0.8608383 ]\n",
      "[-0.18090911 -0.80563164  0.1728695   1.198136  ]\n",
      "[-0.19702175 -1.0025165   0.19683222  1.5396323 ]\n",
      "Episode finished after 24 timesteps\n",
      "[ 0.00760532 -0.04234105 -0.04108422 -0.03324673]\n",
      "[ 0.0067585   0.15334526 -0.04174915 -0.33860385]\n",
      "[ 0.00982541  0.34903565 -0.04852122 -0.6441543 ]\n",
      "[ 0.01680612  0.54479903 -0.06140431 -0.95171344]\n",
      "[ 0.0277021   0.3505548  -0.08043858 -0.67893773]\n",
      "[ 0.0347132   0.15663695 -0.09401733 -0.41262498]\n",
      "[ 0.03784594  0.35295713 -0.10226984 -0.7334051 ]\n",
      "[ 0.04490508  0.15938571 -0.11693794 -0.47458005]\n",
      "[ 0.04809279 -0.03390753 -0.12642954 -0.22092175]\n",
      "[ 0.04741464 -0.22701691 -0.13084798  0.02935898]\n",
      "[ 0.04287431 -0.42004332 -0.1302608   0.2780639 ]\n",
      "[ 0.03447344 -0.6130897  -0.12469952  0.5269917 ]\n",
      "[ 0.02221164 -0.4164541  -0.11415968  0.19776195]\n",
      "[ 0.01388256 -0.21990009 -0.11020444 -0.12864007]\n",
      "[ 0.00948456 -0.41328493 -0.11277725  0.12734292]\n",
      "[ 0.00121886 -0.6066258  -0.11023039  0.3824256 ]\n",
      "[-0.01091365 -0.8000241  -0.10258187  0.6384202 ]\n",
      "[-0.02691413 -0.6036327  -0.08981347  0.31527552]\n",
      "[-0.03898679 -0.40735382 -0.08350796 -0.00432577]\n",
      "[-0.04713386 -0.21113968 -0.08359447 -0.32214415]\n",
      "[-0.05135666 -0.01493299 -0.09003736 -0.6399746 ]\n",
      "[-0.05165532  0.18132123 -0.10283685 -0.9595988 ]\n",
      "[-0.04802889 -0.0122792  -0.12202883 -0.7009131 ]\n",
      "[-0.04827448 -0.20551723 -0.13604708 -0.448999  ]\n",
      "[-0.05238482 -0.398479   -0.14502707 -0.20210537]\n",
      "[-0.0603544  -0.59126127 -0.14906918  0.04154358]\n",
      "[-0.07217963 -0.394351   -0.1482383  -0.29421028]\n",
      "[-0.08006665 -0.58708286 -0.1541225  -0.05170573]\n",
      "[-0.0918083  -0.7796976  -0.15515663  0.18865879]\n",
      "[-0.10740226 -0.9722987  -0.15138344  0.42865676]\n",
      "[-0.12684824 -0.7753934  -0.14281031  0.09233978]\n",
      "[-0.1423561  -0.9682104  -0.14096351  0.33677724]\n",
      "[-0.1617203  -0.7713933  -0.13422798  0.00317378]\n",
      "[-0.17714818 -0.5746272  -0.1341645  -0.3286651 ]\n",
      "[-0.18864071 -0.7676095  -0.1407378  -0.08111917]\n",
      "[-0.2039929  -0.96046287 -0.14236018  0.16406022]\n",
      "[-0.22320217 -1.1532904  -0.13907897  0.40866277]\n",
      "[-0.24626797 -0.95649874 -0.13090572  0.07556815]\n",
      "[-0.26539794 -0.75976676 -0.12939435 -0.2553814 ]\n",
      "[-0.28059328 -0.9528267  -0.13450198 -0.00614834]\n",
      "[-0.2996498  -0.75605786 -0.13462496 -0.3380603 ]\n",
      "[-0.31477097 -0.9490329  -0.14138615 -0.09067709]\n",
      "[-0.33375162 -0.7521973  -0.1431997  -0.4244135 ]\n",
      "[-0.34879556 -0.55536807 -0.15168796 -0.7585924 ]\n",
      "[-0.35990295 -0.35851762 -0.16685982 -1.094903  ]\n",
      "[-0.3670733  -0.55109626 -0.18875788 -0.8588765 ]\n",
      "[-0.3780952  -0.7432151  -0.2059354  -0.63098276]\n",
      "Episode finished after 47 timesteps\n",
      "[-0.04454145  0.0474929  -0.02652899  0.00197185]\n",
      "[-0.04359159 -0.14723873 -0.02648956  0.28616792]\n",
      "[-0.04653637  0.04825078 -0.0207662  -0.0147505 ]\n",
      "[-0.04557135 -0.1465673  -0.02106121  0.27130878]\n",
      "[-0.0485027  -0.34138247 -0.01563503  0.55727524]\n",
      "[-0.05533035 -0.53628147 -0.00448953  0.8449915 ]\n",
      "[-0.06605598 -0.7313419   0.0124103   1.1362592 ]\n",
      "[-0.08068281 -0.53638446  0.03513549  0.8474941 ]\n",
      "[-0.0914105  -0.34175897  0.05208537  0.56606376]\n",
      "[-0.09824568 -0.53757143  0.06340665  0.8746903 ]\n",
      "[-0.10899711 -0.7334954   0.08090045  1.1866149 ]\n",
      "[-0.12366702 -0.5395103   0.10463275  0.9203477 ]\n",
      "[-0.13445723 -0.7358789   0.1230397   1.2439955 ]\n",
      "[-0.14917481 -0.93234587  0.14791961  1.5725505 ]\n",
      "[-0.16782172 -1.1288904   0.17937063  1.9074779 ]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.0362667  -0.02185408  0.03041577  0.04243184]\n",
      "[-0.03670378  0.17281881  0.0312644  -0.24050158]\n",
      "[-0.0332474  -0.02273548  0.02645437  0.06187674]\n",
      "[-0.03370211  0.17199738  0.02769191 -0.22234367]\n",
      "[-0.03026216  0.3667128   0.02324503 -0.50616443]\n",
      "[-0.02292791  0.5614996   0.01312174 -0.7914323 ]\n",
      "[-0.01169791  0.756439   -0.0027069  -1.0799584 ]\n",
      "[ 0.00343087  0.56135285 -0.02430607 -0.78812623]\n",
      "[ 0.01465792  0.7568001  -0.0400686  -1.0883558 ]\n",
      "[ 0.02979393  0.9524268  -0.06183571 -1.3933372 ]\n",
      "[ 0.04884246  0.7581267  -0.08970246 -1.1206123 ]\n",
      "[ 0.06400499  0.5642884  -0.1121147  -0.85736096]\n",
      "[ 0.07529076  0.7607446  -0.12926193 -1.183088  ]\n",
      "[ 0.09050565  0.5675148  -0.15292367 -0.9335585 ]\n",
      "[ 0.10185595  0.3747497  -0.17159484 -0.6925698 ]\n",
      "[ 0.10935094  0.57178426 -0.18544625 -1.0339828 ]\n",
      "[ 0.12078663  0.37954724 -0.2061259  -0.8047806 ]\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.0371992   0.03643104 -0.0456162  -0.00361094]\n",
      "[ 0.03792781 -0.15800804 -0.04568842  0.27433777]\n",
      "[ 0.03476765  0.03773501 -0.04020167 -0.03239813]\n",
      "[ 0.03552235 -0.15678805 -0.04084963  0.24733461]\n",
      "[ 0.03238659  0.03889277 -0.03590294 -0.05794822]\n",
      "[ 0.03316445 -0.1556965  -0.0370619   0.22319438]\n",
      "[ 0.03005052 -0.35026968 -0.03259801  0.50396   ]\n",
      "[ 0.02304512 -0.15470383 -0.02251881  0.20118496]\n",
      "[ 0.01995105 -0.3494966  -0.01849511  0.48668006]\n",
      "[ 0.01296112 -0.15411861 -0.00876151  0.18822597]\n",
      "[ 0.00987874  0.04112758 -0.00499699 -0.10720793]\n",
      "[ 0.0107013  -0.15392241 -0.00714115  0.18389429]\n",
      "[ 0.00762285  0.04130099 -0.00346327 -0.11103283]\n",
      "[ 0.00844887 -0.15377116 -0.00568392  0.18055545]\n",
      "[ 0.00537344 -0.34881133 -0.00207281  0.4714399 ]\n",
      "[-0.00160278 -0.54390395  0.00735598  0.7634688 ]\n",
      "[-0.01248086 -0.73912644  0.02262536  1.0584573 ]\n",
      "[-0.02726339 -0.5443114   0.04379451  0.7729608 ]\n",
      "[-0.03814962 -0.74000764  0.05925372  1.0790952 ]\n",
      "[-0.05294977 -0.5457161   0.08083563  0.8055797 ]\n",
      "[-0.06386409 -0.3517898   0.09694722  0.53937995]\n",
      "[-0.07089989 -0.54813135  0.10773481  0.86096716]\n",
      "[-0.08186252 -0.7445425   0.12495416  1.1854886 ]\n",
      "[-0.09675337 -0.9410437   0.14866392  1.5145832 ]\n",
      "[-0.11557424 -1.1376194   0.1789556   1.8497409 ]\n",
      "Episode finished after 25 timesteps\n",
      "[-0.01691147  0.049363   -0.01973037 -0.04326427]\n",
      "[-0.01592421 -0.14547054 -0.02059566  0.24312882]\n",
      "[-0.01883362  0.04993944 -0.01573308 -0.05597869]\n",
      "[-0.01783483 -0.14495343 -0.01685265  0.2316991 ]\n",
      "[-0.0207339   0.05040522 -0.01221867 -0.06625172]\n",
      "[-0.0197258   0.24570021 -0.01354371 -0.3627645 ]\n",
      "[-0.0148118  0.441012  -0.020799  -0.6596871]\n",
      "[-0.00599156  0.2461856  -0.03399274 -0.37362513]\n",
      "[-0.00106784  0.4417735  -0.04146524 -0.67682946]\n",
      "[ 0.00776763  0.2472515  -0.05500183 -0.39748454]\n",
      "[ 0.01271266  0.4431089  -0.06295152 -0.7069887 ]\n",
      "[ 0.02157483  0.24891292 -0.0770913  -0.4347673 ]\n",
      "[ 0.02655309  0.44503677 -0.08578664 -0.75072175]\n",
      "[ 0.03545383  0.25119612 -0.10080107 -0.48622036]\n",
      "[ 0.04047775  0.05763024 -0.11052548 -0.22693074]\n",
      "[ 0.04163035 -0.13575289 -0.1150641   0.02894561]\n",
      "[ 0.0389153   0.06081505 -0.11448519 -0.2977108 ]\n",
      "[ 0.0401316  -0.13250461 -0.1204394  -0.04321476]\n",
      "[ 0.03748151  0.06412005 -0.1213037  -0.37133884]\n",
      "[ 0.03876391 -0.12908852 -0.12873048 -0.11923152]\n",
      "[ 0.03618214 -0.32215357 -0.13111511  0.13022754]\n",
      "[ 0.02973907 -0.51517737 -0.12851055  0.37884057]\n",
      "[ 0.01943552 -0.31848693 -0.12093374  0.04855963]\n",
      "[ 0.01306578 -0.12185715 -0.11996255 -0.27969882]\n",
      "[ 0.01062864 -0.31508178 -0.12555653 -0.02712938]\n",
      "[ 0.004327   -0.5082004  -0.12609911  0.22345158]\n",
      "[-0.00583701 -0.31152287 -0.12163008 -0.10619728]\n",
      "[-0.01206747 -0.11488687 -0.12375402 -0.43464336]\n",
      "[-0.0143652   0.08174985 -0.1324469  -0.7636336 ]\n",
      "[-0.01273021  0.27842304 -0.14771956 -1.0948852 ]\n",
      "[-0.00716174  0.08552255 -0.16961727 -0.8519575 ]\n",
      "[-0.00545129 -0.10693149 -0.18665642 -0.6170479 ]\n",
      "[-0.00758992  0.09023986 -0.19899738 -0.96222657]\n",
      "Episode finished after 33 timesteps\n",
      "[-0.01728691 -0.03729398  0.03988706 -0.03701702]\n",
      "[-0.01803279 -0.23296455  0.03914672  0.26797906]\n",
      "[-0.02269208 -0.42862266  0.0445063   0.5727476 ]\n",
      "[-0.03126454 -0.62433946  0.05596125  0.87911266]\n",
      "[-0.04375133 -0.82017535  0.07354351  1.1888505 ]\n",
      "[-0.06015483 -0.6260797   0.09732052  0.9200964 ]\n",
      "[-0.07267643 -0.8223727   0.11572245  1.2417097 ]\n",
      "[-0.08912387 -0.6289106   0.14055665  0.98740405]\n",
      "[-0.10170209 -0.82560617  0.16030473  1.3207254 ]\n",
      "[-0.11821421 -1.0223495   0.18671924  1.6589848 ]\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(300):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c77fdb427e7cbc9bc1367dd530fc2b36aacdbbde1ac83c85833b10dfa8b831c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
